{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnyZEU8wWGyHa8kkq2LaSP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RazzaTitian/ADF-X/blob/main/ADF_X.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Data Factory - Experimental\n",
        "\n",
        "ADF-X is an ambitious, groundbreaking project designed to create a highly modular, robust, and complex data generation factory. The primary objective is to produce synthetic data that can fulfil all possible variations, ensuring the data is as accurate and comprehensive as possible. ADF-X is not just a data generator; it's a complete data ecosystem tailored to be highly customizable, allowing users to specify the type of data they need, the number of iterations, and the expected amount of data."
      ],
      "metadata": {
        "id": "UIxvtd-DSZ1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Sampling"
      ],
      "metadata": {
        "id": "rYzeVD42fUXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming OldData_Room1 is your original DataFrame\n",
        "OldData_Room1 = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Perform Simple Random Sampling\n",
        "# The number of samples can be adjusted based on your needs\n",
        "num_samples = 1000  # Adjust this number\n",
        "\n",
        "SRS_Room1 = OldData_Room1.sample(n=num_samples, replace=True, random_state=1)\n",
        "\n",
        "# Now SRS_Room1 contains the randomly sampled data from OldData_Room1\n",
        "\n"
      ],
      "metadata": {
        "id": "HY37KIDVfWlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SMOTE"
      ],
      "metadata": {
        "id": "O1oRzIjMe8LM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize SMOTE\n",
        "smote = SMOTE(sampling_strategy='auto')\n",
        "\n",
        "# Fit on data\n",
        "X, y = SRS_Room1.drop('target_column', axis=1), SRS_Room1['target_column']\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Combine resampled data into a new DataFrame\n",
        "SMOTE_Room1 = pd.concat([X_resampled, y_resampled], axis=1)"
      ],
      "metadata": {
        "id": "EQK-ROBofjbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Perturbation"
      ],
      "metadata": {
        "id": "XryqdsYwh_Zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\"\"\"\n",
        "    This function applies Gaussian noise to each numerical feature in the dataset\n",
        "    to introduce variability.\n",
        "\n",
        "    Parameters:\n",
        "    - data: pd.DataFrame, the dataset to perturb.\n",
        "    - noise_level: float, the standard deviation of the Gaussian noise as a fraction\n",
        "                   of the data's standard deviation.\n",
        "\n",
        "    Returns:\n",
        "    - perturbed_data: pd.DataFrame, the dataset with added Gaussian noise.\n",
        "    \"\"\"\n",
        "\n",
        "def perturb_data(data, noise_level=0.01):\n",
        "    # For each numerical column, apply Gaussian noise\n",
        "    for column in perturbed_data.select_dtypes(include=[np.number]).columns:\n",
        "        # Calculate the standard deviation of the column\n",
        "        std = perturbed_data[column].std()\n",
        "\n",
        "        # Generate Gaussian noise\n",
        "        noise = np.random.normal(0, std * noise_level, size=perturbed_data[column].shape)\n",
        "\n",
        "        # Add the noise to the column\n",
        "        perturbed_data[column] += noise\n",
        "\n",
        "    return perturbed_data\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'SMOTE_Room1' is the DataFrame containing the SMOTEd data on room 1\n",
        "# perturbed_df = perturb_data(SMOTE_Room1)"
      ],
      "metadata": {
        "id": "N_XauEvgksyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN Section"
      ],
      "metadata": {
        "id": "uSedqJs-Snax"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pm5ozfBzJtFp"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Importing necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Reshape, Flatten\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Data Preprocessing\n",
        "# Assuming df is your DataFrame\n",
        "df = pd.read_csv('your_dataset.csv')\n",
        "X_train = df.values\n",
        "\n",
        "# Create logging directory\n",
        "log_dir = 'logs'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)"
      ],
      "metadata": {
        "id": "vGMcxJWNSm83"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}